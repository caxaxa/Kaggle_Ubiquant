{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n","metadata":{}},{"cell_type":"markdown","source":"# Forked from Ubiquant Codes ---- ensembled with my models","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport gc\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\nimport mlcrate as mlc\nimport pickle as pkl\nfrom tensorflow.keras.layers import BatchNormalization\nfrom keras.models import Sequential, Model\nfrom keras.layers import Input, Embedding, Dense, Flatten, Concatenate, Dot, Reshape, Add, Subtract\nfrom keras import backend as K\nfrom keras import regularizers \nfrom tensorflow.keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.regularizers import l2\nfrom sklearn.base import clone\nfrom typing import Dict\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom tensorflow.keras.losses import Loss\nfrom tensorflow.keras import backend as K\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import TimeSeriesSplit, StratifiedKFold, KFold, GroupKFold\nfrom tqdm import tqdm\nfrom tensorflow.python.ops import math_ops\nimport pickle\nimport math","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-19T13:43:44.608431Z","iopub.execute_input":"2022-04-19T13:43:44.609218Z","iopub.status.idle":"2022-04-19T13:43:52.011804Z","shell.execute_reply.started":"2022-04-19T13:43:44.609103Z","shell.execute_reply":"2022-04-19T13:43:52.011011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_memory_usage(df):\n    features = [f'f_{i}' for i in range(300)]\n    for column in features + [\"target\"]:\n        item = df[column].astype(np.float16)\n        df[column] = item\n        del item\n        gc.collect()\n    for column in [\"time_id\", \"investment_id\"]:\n        item = df[column].astype(np.uint16)\n        df[column] = item\n        del item\n        gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:43:52.013708Z","iopub.execute_input":"2022-04-19T13:43:52.014027Z","iopub.status.idle":"2022-04-19T13:43:52.021933Z","shell.execute_reply.started":"2022-04-19T13:43:52.013984Z","shell.execute_reply":"2022-04-19T13:43:52.021046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nsupplemental_train = pd.read_csv('../input/ubiquant-market-prediction/supplemental_train.csv')\nsupplemental_train.pop(\"row_id\");\nsupplemental_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:43:52.023143Z","iopub.execute_input":"2022-04-19T13:43:52.023453Z","iopub.status.idle":"2022-04-19T13:45:55.424038Z","shell.execute_reply.started":"2022-04-19T13:43:52.023411Z","shell.execute_reply":"2022-04-19T13:45:55.423044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nreduce_memory_usage(supplemental_train)\nsupplemental_train.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:45:55.426366Z","iopub.execute_input":"2022-04-19T13:45:55.426633Z","iopub.status.idle":"2022-04-19T13:48:34.619552Z","shell.execute_reply.started":"2022-04-19T13:45:55.426601Z","shell.execute_reply":"2022-04-19T13:48:34.618886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nn_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]\nfeature_columns = ['investment_id', 'time_id'] + features\ntrain = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')\ntrain.head()\n","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:48:34.620478Z","iopub.execute_input":"2022-04-19T13:48:34.621201Z","iopub.status.idle":"2022-04-19T13:48:51.017051Z","shell.execute_reply.started":"2022-04-19T13:48:34.621168Z","shell.execute_reply":"2022-04-19T13:48:51.016088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"investment_id = train.pop(\"investment_id\")\ninvestment_id.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:48:51.018368Z","iopub.execute_input":"2022-04-19T13:48:51.018669Z","iopub.status.idle":"2022-04-19T13:48:51.031658Z","shell.execute_reply.started":"2022-04-19T13:48:51.018576Z","shell.execute_reply":"2022-04-19T13:48:51.030587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.append(supplemental_train)\ntrain.sort_values(by=\"time_id\", inplace=True)\ndel supplemental_train\ngc.collect()\ntrain.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:48:51.032944Z","iopub.execute_input":"2022-04-19T13:48:51.033185Z","iopub.status.idle":"2022-04-19T13:49:05.610761Z","shell.execute_reply.started":"2022-04-19T13:48:51.033157Z","shell.execute_reply":"2022-04-19T13:49:05.609689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:49:05.612231Z","iopub.execute_input":"2022-04-19T13:49:05.612565Z","iopub.status.idle":"2022-04-19T13:49:05.646233Z","shell.execute_reply.started":"2022-04-19T13:49:05.61252Z","shell.execute_reply":"2022-04-19T13:49:05.645142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_ = train.pop(\"time_id\")\ny = train.pop(\"target\")\ny.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:49:05.647784Z","iopub.execute_input":"2022-04-19T13:49:05.648308Z","iopub.status.idle":"2022-04-19T13:49:05.668894Z","shell.execute_reply.started":"2022-04-19T13:49:05.648256Z","shell.execute_reply":"2022-04-19T13:49:05.668207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ninvestment_ids = list(investment_id.unique())\ninvestment_id_size = len(investment_ids) + 1\ninvestment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\nwith tf.device(\"cpu\"):\n    investment_id_lookup_layer.adapt(investment_id)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:49:05.671564Z","iopub.execute_input":"2022-04-19T13:49:05.672292Z","iopub.status.idle":"2022-04-19T13:49:49.197452Z","shell.execute_reply.started":"2022-04-19T13:49:05.672253Z","shell.execute_reply":"2022-04-19T13:49:49.196469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(X, y):\n    print(X)\n    print(y)\n    return X, y\ndef make_dataset(feature, investment_id, y, batch_size=1024, mode=\"train\"):\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature), y))\n    ds = ds.map(preprocess)\n    if mode == \"train\":\n        ds = ds.shuffle(256)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:49:49.199629Z","iopub.execute_input":"2022-04-19T13:49:49.200225Z","iopub.status.idle":"2022-04-19T13:49:49.207972Z","shell.execute_reply.started":"2022-04-19T13:49:49.200177Z","shell.execute_reply":"2022-04-19T13:49:49.20709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model():\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    return model\n\n\ndef get_model2():\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)    \n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n   # investment_id_x = layers.Dropout(0.65)(investment_id_x)\n   \n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dropout(0.65)(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n   # x = layers.Dropout(0.2)(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n  #  x = layers.Dropout(0.4)(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.75)(x)\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    return model\n\n\ndef get_model3():\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float32)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dropout(0.5)(investment_id_x)\n    investment_id_x = layers.Dense(32, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dropout(0.5)(investment_id_x)\n    #investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dropout(0.5)(feature_x)\n    feature_x = layers.Dense(128, activation='swish')(feature_x)\n    feature_x = layers.Dropout(0.5)(feature_x)\n    feature_x = layers.Dense(64, activation='swish')(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(64, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(16, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.5)(x)\n    output = layers.Dense(1)(x)\n    output = tf.keras.layers.BatchNormalization(axis=1)(output)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    return model\n\ndef get_model5():\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    ## feature ##\n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dropout(0.1)(feature_x)\n    ## convolution 1 ##\n    feature_x = layers.Reshape((-1,1))(feature_x)\n    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=1, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 2 ##\n    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=4, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 3 ##\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=1, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 4 ##\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=4, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## convolution 5 ##\n    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=2, padding='same')(feature_x)\n    feature_x = layers.BatchNormalization()(feature_x)\n    feature_x = layers.LeakyReLU()(feature_x)\n    ## flatten ##\n    feature_x = layers.Flatten()(feature_x)\n    \n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(feature_x)\n    \n    x = layers.Dropout(0.1)(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.1)(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.1)(x)\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    return model\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:49:49.209466Z","iopub.execute_input":"2022-04-19T13:49:49.209734Z","iopub.status.idle":"2022-04-19T13:49:49.457646Z","shell.execute_reply.started":"2022-04-19T13:49:49.209705Z","shell.execute_reply":"2022-04-19T13:49:49.456744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = []\n\nfor i in range(5):\n    model = get_model()\n    model.load_weights(f'../input/dnn-baseline/model_{i}')\n    models.append(model)\n\nfor i in range(10):\n    model = get_model2()\n    model.load_weights(f'../input/tran-dnn-v2-10-fold/model_{i}')\n    models.append(model)\n    \n    \nfor i in range(10):\n    model = get_model3()\n    model.load_weights(f'../input/dnn-newmodel/model_{i}')\n    models.append(model)\n    \n    \nmodels2 = []\n    \nfor i in range(5):\n    model = get_model5()\n    model.load_weights(f'../input/ubiquant-spatial/model_{i}.tf')\n    models2.append(model)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:49:49.459061Z","iopub.execute_input":"2022-04-19T13:49:49.459464Z","iopub.status.idle":"2022-04-19T13:50:01.217483Z","shell.execute_reply.started":"2022-04-19T13:49:49.459419Z","shell.execute_reply":"2022-04-19T13:50:01.216634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model_dr04():\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float32)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dropout(0.4)(feature_x)\n    feature_x = layers.Dense(128, activation='swish')(feature_x)\n    feature_x = layers.Dropout(0.4)(feature_x)\n    feature_x = layers.Dense(64, activation='swish')(feature_x)\n    \n    x = layers.Concatenate(axis=1)([feature_x])\n    x = layers.Dropout(0.4)(x)\n    x = layers.Dense(64, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.4)(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.4)(x)\n    x = layers.Dense(16, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dropout(0.4)(x)\n    output = layers.Dense(1)(x)\n    output = tf.keras.layers.BatchNormalization(axis=1)(output)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001),  loss = correlationLoss, metrics=[correlationMetric])\n    return model\n\ndr=0.3\n\ngpus = tf.config.experimental.list_physical_devices('GPU')\nfor gpu in gpus:\n    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)\n\nn_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]\n\ndef preprocess(X, y):\n    return X, y\ndef make_dataset(feature, y, batch_size=1024, mode=\"train\"):\n    ds = tf.data.Dataset.from_tensor_slices((feature, y))\n    ds = ds.map(preprocess)\n    if mode == \"train\":\n        ds = ds.shuffle(512)\n#     ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    ds = ds.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\n\ndef correlationMetric(x, y, axis=-2):\n    \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n    x = tf.convert_to_tensor(x)\n    y = math_ops.cast(y, x.dtype)\n    n = tf.cast(tf.shape(x)[axis], x.dtype)\n    xsum = tf.reduce_sum(x, axis=axis)\n    ysum = tf.reduce_sum(y, axis=axis)\n    xmean = xsum / n\n    ymean = ysum / n\n    xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n    yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n    corr = cov / tf.sqrt(xvar * yvar)\n    return tf.constant(1.0, dtype=x.dtype) - corr\n\n\ndef correlationLoss(x,y, axis=-2):\n    \"\"\"Loss function that maximizes the pearson correlation coefficient between the predicted values and the labels,\n    while trying to have the same mean and variance\"\"\"\n    x = tf.convert_to_tensor(x)\n    y = math_ops.cast(y, x.dtype)\n    n = tf.cast(tf.shape(x)[axis], x.dtype)\n    xsum = tf.reduce_sum(x, axis=axis)\n    ysum = tf.reduce_sum(y, axis=axis)\n    xmean = xsum / n\n    ymean = ysum / n\n    xsqsum = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n    ysqsum = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n    corr = cov / tf.sqrt(xsqsum * ysqsum)\n    return tf.convert_to_tensor( K.mean(tf.constant(1.0, dtype=x.dtype) - corr ) , dtype=tf.float32 )\n\ndef correlationMetric_01mse(x, y, axis=-2):\n    \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n    x = tf.convert_to_tensor(x)\n    y = math_ops.cast(y, x.dtype)\n    n = tf.cast(tf.shape(x)[axis], x.dtype)\n    xsum = tf.reduce_sum(x, axis=axis)\n    ysum = tf.reduce_sum(y, axis=axis)\n    xmean = xsum / n\n    ymean = ysum / n\n    xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n    yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n    cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n    corr = cov / tf.sqrt(xvar * yvar)\n    return tf.constant(1.0, dtype=x.dtype) - corr\n\ngc.collect()\n\n# list(GroupKFold(5).split(train , groups = train.index))[0]\ndef pearson_coef(data):\n    return data.corr()['target']['preds']\n\ndef evaluate_metric(valid_df):\n    return np.mean(valid_df[['time_id_', 'target', 'preds']].groupby('time_id').apply(pearson_coef))\n\n \nmodels3 = []\n\nfor i in range(10):\n    model = get_model_dr04()\n    model.load_weights(f'../input/weights-1/tss_weights/model_{i}')\n    models3.append(model)\n    \n    \n","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:50:01.218805Z","iopub.execute_input":"2022-04-19T13:50:01.219028Z","iopub.status.idle":"2022-04-19T13:50:04.011551Z","shell.execute_reply.started":"2022-04-19T13:50:01.219001Z","shell.execute_reply":"2022-04-19T13:50:04.010436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"investment_id[~investment_id.isin([85, 905, 2558, 3662, 2800, 1415])]","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:50:04.012992Z","iopub.execute_input":"2022-04-19T13:50:04.014813Z","iopub.status.idle":"2022-04-19T13:50:04.050098Z","shell.execute_reply.started":"2022-04-19T13:50:04.014745Z","shell.execute_reply":"2022-04-19T13:50:04.049131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"investment_id2 = investment_id[~investment_id.isin([85, 905, 2558, 3662, 2800, 1415])]\n\ninvestment_ids2 = list(investment_id2.unique())\ninvestment_id_size2 = len(investment_ids2) + 1\ninvestment_id_lookup_layer2 = layers.IntegerLookup(max_tokens=investment_id_size2)\ninvestment_id_lookup_layer2.adapt(pd.DataFrame({\"investment_ids\":investment_ids}))","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:50:04.051516Z","iopub.execute_input":"2022-04-19T13:50:04.051814Z","iopub.status.idle":"2022-04-19T13:50:04.240763Z","shell.execute_reply.started":"2022-04-19T13:50:04.051756Z","shell.execute_reply":"2022-04-19T13:50:04.240088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef get_model_best(ft_units, x_units, x_dropout):\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer2(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size2, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(128, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(128, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(128, activation='swish')(investment_id_x)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    for hu in ft_units:\n        feature_x = layers.Dense(hu, activation='swish')(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    \n    for i in range(len(x_units)):\n        x = tf.keras.layers.Dense(x_units[i], kernel_regularizer=\"l2\")(x) #v8\n        x = tf.keras.layers.BatchNormalization()(x) #v7\n        x = tf.keras.layers.Activation('swish')(x) #v7\n        x = tf.keras.layers.Dropout(x_dropout[i])(x) #v8\n        \n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.0001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    return model\n\nparams = {\n    'ft_units': [256,256],\n    'x_units': [512, 256, 128, 32],\n    'x_dropout': [0.4, 0.3, 0.2, 0.1]\n#           'lr':1e-3, \n         }\n\n# models_best = []\n# scores = []\n# for i in range(7):\n#     model = get_model_best(**params)\n#     model.load_weights(f'../input/ump-conv-del-idx-outputs/model_{i}.tf')\n#     models_best.append(model)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:50:04.242066Z","iopub.execute_input":"2022-04-19T13:50:04.242538Z","iopub.status.idle":"2022-04-19T13:50:04.258677Z","shell.execute_reply.started":"2022-04-19T13:50:04.242494Z","shell.execute_reply":"2022-04-19T13:50:04.257611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_model_corr(ft_units, x_units, x_dropout):\n    \n    # investment_id\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(128, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(128, activation='swish')(investment_id_x) \n    investment_id_x = layers.Dense(128, activation='swish')(investment_id_x)\n    \n    # features_inputs\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    bn = tf.keras.layers.BatchNormalization()(features_inputs)\n    gn = tf.keras.layers.GaussianNoise(0.035)(bn)\n    feature_x = layers.Dense(300, activation='swish')(gn)\n    feature_x = tf.keras.layers.Dropout(0.5)(feature_x)\n    \n    for hu in ft_units:\n        feature_x = layers.Dense(hu, activation='swish')(feature_x)\n#         feature_x = tf.keras.layers.Activation('swish')(feature_x)\n        feature_x = tf.keras.layers.Dropout(0.35)(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    \n    for i in range(len(x_units)):\n        x = tf.keras.layers.Dense(x_units[i], kernel_regularizer=\"l2\")(x) \n        x = tf.keras.layers.Activation('swish')(x)\n        x = tf.keras.layers.Dropout(x_dropout[i])(x)\n        \n    output = layers.Dense(1)(x)\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.0001), loss=correlationLoss, \n                  metrics=['mse', \"mae\", correlation])\n    return model\n\n\nparams = {\n#     'num_columns': len(features), \n    'ft_units': [150, 75, 150 ,200],\n    'x_units': [512, 256, 128, 32],\n    'x_dropout': [0.44, 0.4, 0.33, 0.2] #4, 3, 2, 1\n#           'lr':1e-3, \n         }","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:50:04.260415Z","iopub.execute_input":"2022-04-19T13:50:04.261433Z","iopub.status.idle":"2022-04-19T13:50:04.281384Z","shell.execute_reply.started":"2022-04-19T13:50:04.261362Z","shell.execute_reply":"2022-04-19T13:50:04.280495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del train\ndel investment_id\ndel y","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:50:04.282983Z","iopub.execute_input":"2022-04-19T13:50:04.283454Z","iopub.status.idle":"2022-04-19T13:50:04.30347Z","shell.execute_reply.started":"2022-04-19T13:50:04.283356Z","shell.execute_reply":"2022-04-19T13:50:04.302713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_test(investment_id, feature):\n    return (investment_id, feature), 0\n\ndef preprocess_test_s(feature):\n    return (feature), 0\n\ndef make_test_dataset(feature, investment_id, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n    ds = ds.map(preprocess_test)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\n\ndef make_test_dataset2(feature, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices(((feature)))\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n    return ds\n\ndef inference(models, ds):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=1)\n\n\ndef make_test_dataset3(feature, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices((feature))\n    ds = ds.map(preprocess_test_s)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\n\ndef infer(models, ds):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds)\n        y_preds.append((y_pred-y_pred.mean())/y_pred.std())\n    \n    return np.mean(y_preds, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:50:04.304836Z","iopub.execute_input":"2022-04-19T13:50:04.305463Z","iopub.status.idle":"2022-04-19T13:50:04.346693Z","shell.execute_reply.started":"2022-04-19T13:50:04.305422Z","shell.execute_reply":"2022-04-19T13:50:04.345651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1 = pickle.load(open('../input/ubiquant-scikitgb-final/GB_2.sav', 'rb'))\nmodel_2 = pickle.load(open('../input/ubiquant-simple-lgboost-final/simple_LGBM_3.sav', 'rb'))\nmodel_3 = pickle.load(open('../input/ubiquant-simple-xgboost-final/simple_xgb_3.sav', 'rb'))\nmodel_4 = keras.models.load_model('../input/ubiqueant-forked-dnn-1/keras_model.h5')","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:50:04.348253Z","iopub.execute_input":"2022-04-19T13:50:04.348575Z","iopub.status.idle":"2022-04-19T13:50:12.947878Z","shell.execute_reply.started":"2022-04-19T13:50:04.348535Z","shell.execute_reply":"2022-04-19T13:50:12.947173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:50:12.949894Z","iopub.execute_input":"2022-04-19T13:50:12.950489Z","iopub.status.idle":"2022-04-19T13:50:12.955414Z","shell.execute_reply.started":"2022-04-19T13:50:12.950441Z","shell.execute_reply":"2022-04-19T13:50:12.954474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test() \nfor (test_df, sample_prediction_df) in iter_test:\n    ds = make_test_dataset(test_df[features], test_df[\"investment_id\"])\n    p1 = inference(models, ds)\n    ds2 = make_test_dataset2(test_df[features])\n    p2 = inference(models2, ds2)\n    ds3 = make_test_dataset3(test_df[features])\n    p3 = infer(models3, ds3)\n    #p4 = inference(models_best, ds)\n    X = test_df.iloc[:,1:302].values\n    X = scaler.fit_transform(X)\n    p5 = model_1.predict(scaler.fit_transform(test_df.iloc[:,2:299].values))\n    p6 = model_2.predict(X)\n    p7 = model_3.predict(X)\n    p8 = np.concatenate(model_4.predict(X))\n    sample_prediction_df['target'] =p1 * 1.1 + p2 * 0.25 + p3 * 0.05 + p5 * 0.45 + p6* 0.01 + p7* 0.03 + p8* 0.05\n    env.predict(sample_prediction_df) \n    display(sample_prediction_df)","metadata":{"execution":{"iopub.status.busy":"2022-04-19T13:50:12.956876Z","iopub.execute_input":"2022-04-19T13:50:12.957277Z","iopub.status.idle":"2022-04-19T13:50:24.372314Z","shell.execute_reply.started":"2022-04-19T13:50:12.957229Z","shell.execute_reply":"2022-04-19T13:50:24.371461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}